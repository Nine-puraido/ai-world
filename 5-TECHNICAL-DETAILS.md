# AI World â€” Technical Communication Spec

## What Happens When You Send a Message (Full Infrastructure Flow)

You send a Telegram message. Here's what physically happens on your VM.

### What's always running (24/7, even when idle):

```
YOUR VM
â”œâ”€â”€ API Server          (~50-100MB)   â† listens for Telegram webhooks
â”œâ”€â”€ Router              (~30-50MB)    â† routes messages to agent containers (no brain, no tokens)
â”œâ”€â”€ Jarvis Agent        (~100-200MB)  â† Planner + Policy + Broker + Verifier + own 1Password SA
â”œâ”€â”€ Nova Agent          (~100-200MB)  â† own container, own 1Password SA
â”œâ”€â”€ Atlas Agent         (~100-200MB)  â† own container, own 1Password SA
â”œâ”€â”€ Agent Manager       (~30-50MB)    â† create/edit/delete agents (CRITICAL, human-only)
â”œâ”€â”€ Worker Manager      (~30-50MB)    â† lightweight, just listens for spawn requests (shared)
â”œâ”€â”€ Redis               (~30-50MB)    â† message bus
â””â”€â”€ PostgreSQL          (~100-200MB)  â† state + audit log

Idle RAM: ~600-1000MB total (scales with number of agents)
```

Worker containers are **OFF**. They don't exist until a task needs them.

### The flow:

```
1. YOU TYPE ON TELEGRAM
   "Build me a todo app with auth"
         â”‚
         â–¼
2. TELEGRAM SERVERS â†’ YOUR VM (webhook)
   Telegram pushes the message to your API Server via HTTPS.
   Your server doesn't poll â€” Telegram knocks on your door.
         â”‚
         â–¼
3. API SERVER (always running)
   Checks: is this from your Telegram user ID? âœ…
   Creates a goal in PostgreSQL.
   Publishes to Redis Stream â†’ "goals:incoming"
         â”‚
         â–¼
4. ROUTER (always running, listening on Redis)
   Looks at the goal â†’ which agent should handle this?
   Routes to the correct agent container (e.g. Jarvis).
   Router has NO brain, NO tokens â€” just routing logic.
         â”‚
         â–¼
5. JARVIS AGENT CONTAINER (always running, listening on Redis)
   Jarvis's Planner calls LLM (Anthropic API â†’ Claude Opus).
   "Break this goal into tasks..."
   Returns: 5 tasks with dependency graph.
   Policy Engine checks each task â†’ approved.
   Broker creates capability leases in PostgreSQL.
   Jarvis resolves secrets via its own 1Password Service Account.

   (No worker containers spawned yet. This is all inside Jarvis's container.)
         â”‚
         â–¼
6. WORKER CONTAINERS WAKE UP
   Jarvis Agent â†’ Worker Manager: "Spawn worker for task-1"
   Worker Manager: docker run --rm worker-image ...
   Container starts (~2 seconds).

   Worker (task-1):
     Reads assignment from Redis.
     Calls LLM via tool proxy (agent container makes the API call).
     Writes code to /workspace/task-1/.
     Sends heartbeats every 10s.
     Finishes â†’ sends task.complete â†’ container auto-destroyed.
         â”‚
         â–¼
7. REPEAT PER WAVE
   task-1 done â†’ Verifier checks (inside Jarvis container, calls GPT-4o)
              â†’ merged to main
              â†’ task-2 unblocked â†’ new worker container spawns â†’ works â†’ dies
              â†’ ...until all tasks done...
         â”‚
         â–¼
8. RESULT GOES BACK TO YOU
   Jarvis Agent composes summary â†’ Redis â†’ Router â†’ API Server
   API Server calls Telegram Bot API â†’ sendMessage
   Telegram servers â†’ your phone â†’ notification pops up

   "âœ… Deployed! Frontend: https://... API: https://..."
```

### Key insight

Jarvis doesn't "wake up." Each agent container is always running, always listening.
The Router is always running, routing messages to the right agent. What wakes up
and dies are the **workers** â€” they get spawned per task and destroyed when done.
The agents stay at their desks 24/7. The Router just opens the right door.

---

## Transport + Worker Logic

**Approach 3** (Claude API tool loop) for worker logic â€” maximum control over tool execution.
**Approach 2** (Redis Streams) for transport/orchestration â€” durable, async, container-friendly.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ROUTER  â”‚  â† routes messages, no brain, no tokens
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JARVIS AGENT â”‚  â”‚ NOVA AGENT   â”‚ â”‚ ATLAS AGENT  â”‚  â† each in own container
â”‚              â”‚  â”‚              â”‚ â”‚              â”‚    with own 1Password SA
â”‚ Planner      â”‚  â”‚ Planner      â”‚ â”‚ Planner      â”‚
â”‚ Policy       â”‚  â”‚ Policy       â”‚ â”‚ Policy       â”‚
â”‚ Broker       â”‚  â”‚ Broker       â”‚ â”‚ Broker       â”‚
â”‚ Verifier     â”‚  â”‚ Verifier     â”‚ â”‚ Verifier     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚  Redis Streams (durable)
        â”‚  task:{id}:in / task:{id}:out
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WORKER          â”‚  â† ephemeral container
â”‚  (API tool loop) â”‚
â”‚  Claude API +    â”‚    LLM calls proxied through
â”‚  sandboxed tools â”‚    agent container (worker has no API key)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Agent Lifecycle Management

The Agent Manager handles creating, editing, and deleting agent containers.
This is a **CRITICAL action** â€” only a human from the Dashboard can trigger it.
No agent can create another agent.

### How Agent Manager Creates a New Agent

```
Human clicks [Create Agent] in Dashboard
  â”‚
  â–¼
Agent Manager:
  â”‚ 1. Validates config (name, role, models, capabilities, budget)
  â”‚ 2. Creates a new 1Password Vault for the agent (if needed)
  â”‚ 3. Creates a 1Password Service Account scoped to that vault (read-only)
  â”‚ 4. Stores the Service Account token as a container secret
  â”‚ 5. Spins up a new Docker container with:
  â”‚      â€¢ Planner + Policy Engine + Broker + Verifier
  â”‚      â€¢ The agent's 1Password Service Account token
  â”‚      â€¢ Agent config (models, capabilities, budget)
  â”‚ 6. Registers the agent with the Router (so messages can be routed to it)
  â”‚ 7. Registers the agent in PostgreSQL (for tracking + Dashboard)
  â”‚
  â–¼
New agent container is running and ready to receive goals via Router
```

### Agent Lifecycle State Machine

```
CREATING â†’ STARTING â†’ RUNNING â†’ STOPPING â†’ STOPPED
                â”‚         â”‚         â”‚
                â”‚         â”‚         â””â”€â”€â–º DELETING â†’ DELETED
                â”‚         â”‚
                â”‚         â”œâ”€â”€â–º RESTARTING â†’ RUNNING
                â”‚         â”‚
                â”‚         â””â”€â”€â–º ERROR (container crash / health check fail)
                â”‚                â”‚
                â”‚                â””â”€â”€â–º RESTARTING â†’ RUNNING (auto-restart)
                â”‚
                â””â”€â”€â–º CREATE_FAILED (container didn't start / SA creation failed)
```

### What Agent Manager Controls

```
Create agent:   vault + SA + container + Router registration
Edit agent:     hot-reload config (models, budget, capabilities) without restart
Start agent:    start a stopped container (tasks resume from where they paused)
Stop agent:     gracefully stop container (pause active tasks, keep state)
Restart agent:  stop + start (useful after config changes that need restart)
Delete agent:   stop all tasks â†’ destroy container â†’ revoke SA â†’ deregister from Router
```

### Security Rules

- **Human-only**: Agent Manager only accepts requests from the Dashboard
  authenticated as the human owner. No API or agent can call it.
- **No self-replication**: No agent can call Agent Manager. This is enforced
  at the network level (agents cannot reach Agent Manager's port) and at the
  auth level (requires human session token).
- **Audit trail**: Every Agent Manager action is logged to PostgreSQL with
  the human's identity, timestamp, and full action details.

---

## Message Envelope (Strict, Always)

Every single message between Master and Worker uses this envelope.
No exceptions. Without correlation IDs, async flows break.

```typescript
interface MessageEnvelope {
  id: string;            // unique message ID (UUIDv7 â€” time-sortable)
  taskId: string;        // which task this belongs to
  type: MessageType;     // what kind of message
  replyTo: string | null;// ID of the message this replies to (correlation)
  ts: string;            // ISO 8601 timestamp
  idempotencyKey: string;// prevent duplicate processing on retry
  signature: string;     // HMAC-SHA256 of (id:taskId:type:ts) â€” prevents forgery
  payload: object;       // type-specific data
}

type MessageType =
  // Master â†’ Worker
  | "task.assign"          // here's your task
  | "task.cancel"          // stop immediately
  | "answer"               // response to a worker question
  | "permission.grant"     // you may proceed
  | "permission.deny"      // you may not proceed
  | "capability.lease"     // here's your scoped lease

  // Worker â†’ Master
  | "progress"             // status update
  | "question"             // need Master's decision
  | "permission.request"   // need more permissions
  | "heartbeat"            // I'm still alive
  | "artifact"             // structured output (diff, test result, etc.)
  | "task.complete"        // I'm done
  | "task.failed"          // I failed, here's why
```

### Example: Full message exchange

```json
// 1. Master assigns task
{
  "id": "msg-001",
  "taskId": "task-42",
  "type": "task.assign",
  "replyTo": null,
  "ts": "2026-02-09T10:00:00.000Z",
  "idempotencyKey": "idem-task-42-assign",
  "payload": {
    "prompt": "Build a REST API with Express and PostgreSQL for a todo app",
    "branch": "task-42",
    "capabilities": ["pkg-install", "build", "test", "git-commit"],
    "lease": {
      "leaseId": "lease-abc-123",
      "expiresAt": "2026-02-09T10:30:00.000Z",
      "maxTokens": 100000,
      "maxDurationMinutes": 30
    }
  }
}

// 2. Worker sends heartbeat
{
  "id": "msg-002",
  "taskId": "task-42",
  "type": "heartbeat",
  "replyTo": null,
  "ts": "2026-02-09T10:00:15.000Z",
  "idempotencyKey": "idem-task-42-hb-1",
  "payload": {
    "status": "working",
    "currentStep": "Scaffolding project structure",
    "tokenUsage": 1240,
    "costUsd": 0.02
  }
}

// 3. Worker requests permission
{
  "id": "msg-003",
  "taskId": "task-42",
  "type": "permission.request",
  "replyTo": null,
  "ts": "2026-02-09T10:01:22.000Z",
  "idempotencyKey": "idem-task-42-perm-001",
  "payload": {
    "capability": "pkg-install",
    "detail": {
      "packages": ["express", "pg", "dotenv"],
      "registry": "registry.npmjs.org"
    },
    "reason": "Need Express for HTTP server and pg for PostgreSQL client"
  }
}

// 4. Master grants (after policy check)
{
  "id": "msg-004",
  "taskId": "task-42",
  "type": "permission.grant",
  "replyTo": "msg-003",
  "ts": "2026-02-09T10:01:22.500Z",
  "idempotencyKey": "idem-task-42-perm-001-grant",
  "payload": {
    "capability": "pkg-install",
    "approvedBy": "policy-engine",
    "note": "Matches capability class pkg-install, all packages from approved registry"
  }
}

// 5. Worker asks a question
{
  "id": "msg-005",
  "taskId": "task-42",
  "type": "question",
  "replyTo": null,
  "ts": "2026-02-09T10:03:45.000Z",
  "idempotencyKey": "idem-task-42-q-001",
  "payload": {
    "question": "The task says REST API but should I also add input validation middleware?",
    "context": "Building the route handlers now. Could use express-validator or zod.",
    "options": ["Yes, use zod", "Yes, use express-validator", "No, skip for now"],
    "urgency": "blocking"
  }
}

// 6. Master answers (Planner AI decided)
{
  "id": "msg-006",
  "taskId": "task-42",
  "type": "answer",
  "replyTo": "msg-005",
  "ts": "2026-02-09T10:03:46.200Z",
  "idempotencyKey": "idem-task-42-q-001-answer",
  "payload": {
    "answer": "Yes, use zod. It gives type-safe validation and works well with TypeScript if we migrate later.",
    "decidedBy": "planner-ai"
  }
}

// 7. Worker returns structured artifact
{
  "id": "msg-007",
  "taskId": "task-42",
  "type": "artifact",
  "replyTo": null,
  "ts": "2026-02-09T10:12:30.000Z",
  "idempotencyKey": "idem-task-42-artifact-001",
  "payload": {
    "artifactType": "test_result",
    "data": {
      "framework": "jest",
      "exitCode": 0,
      "passed": 12,
      "failed": 0,
      "skipped": 1,
      "coverage": 87.3,
      "durationMs": 4230,
      "logFile": "/workspace/task-42/test-output.log"
    }
  }
}

// 8. Worker completes
{
  "id": "msg-008",
  "taskId": "task-42",
  "type": "task.complete",
  "replyTo": "msg-001",
  "ts": "2026-02-09T10:14:00.000Z",
  "idempotencyKey": "idem-task-42-complete",
  "payload": {
    "summary": "REST API built with Express + PostgreSQL + Zod validation",
    "branch": "task-42",
    "artifacts": {
      "files_changed": [
        {"path": "src/index.js", "action": "created", "lines": 84},
        {"path": "src/routes/todos.js", "action": "created", "lines": 120},
        {"path": "src/db/schema.sql", "action": "created", "lines": 32},
        {"path": "src/middleware/validate.js", "action": "created", "lines": 28},
        {"path": "package.json", "action": "created", "lines": 22},
        {"path": "tests/api.test.js", "action": "created", "lines": 95}
      ],
      "diff_stats": {
        "additions": 381,
        "deletions": 0,
        "files_changed": 6
      },
      "test_results": {
        "passed": 12,
        "failed": 0,
        "coverage": 87.3
      },
      "build_result": {
        "exitCode": 0,
        "durationMs": 2100
      }
    },
    "costs": {
      "llmTokensUsed": 42380,
      "llmCostUsd": 0.68,
      "apiCallsMade": 8,
      "wallTimeSeconds": 840
    }
  }
}
```

---

## Worker Liveness Controls

Workers can hang, crash, or go rogue. Master needs to detect and handle this.

### Heartbeat Protocol

```
Worker sends heartbeat every 10 seconds.
Master expects heartbeat within 30 seconds (3x grace period).

Timeline:
  0s    Worker sends heartbeat     âœ… alive
  10s   Worker sends heartbeat     âœ… alive
  20s   Worker sends heartbeat     âœ… alive
  30s   No heartbeat...            âš ï¸ warning
  40s   No heartbeat...            ğŸ”´ lease_expired â†’ kill worker
```

```yaml
liveness:
  heartbeat_interval_seconds: 10
  heartbeat_timeout_seconds: 30     # 3x interval
  actions_on_timeout:
    - revoke_lease
    - kill_container
    - mark_task_failed
    - log_incident
    - retry_if_under_cap
```

### Worker Lifecycle State Machine

```
SPAWNING â†’ READY â†’ WORKING â†’ COMPLETING â†’ DONE
              â”‚        â”‚          â”‚
              â”‚        â”‚          â””â”€â”€â–º FAILED (worker error)
              â”‚        â”‚
              â”‚        â”œâ”€â”€â–º BLOCKED (waiting for answer/permission)
              â”‚        â”‚       â”‚
              â”‚        â”‚       â””â”€â”€â–º WORKING (answer received)
              â”‚        â”‚
              â”‚        â”œâ”€â”€â–º CANCELLED (Master sent task.cancel)
              â”‚        â”‚
              â”‚        â””â”€â”€â–º TIMED_OUT (lease expired / heartbeat missed)
              â”‚
              â””â”€â”€â–º SPAWN_FAILED (container didn't start)
```

### Lease Expiry Enforcement

```python
# Master-side lease monitor (runs every 5 seconds)

async def monitor_leases():
    while True:
        active_leases = db.get_active_leases()

        for lease in active_leases:
            # Check time expiry
            if now() > lease.expires_at:
                await handle_lease_expired(lease, reason="time_expired")
                continue

            # Check token budget
            if lease.tokens_used > lease.max_tokens:
                await handle_lease_expired(lease, reason="token_budget_exceeded")
                continue

            # Check cost budget
            if lease.cost_usd > lease.max_cost_usd:
                await handle_lease_expired(lease, reason="cost_budget_exceeded")
                continue

            # Check heartbeat
            last_hb = lease.last_heartbeat_at
            if now() - last_hb > timedelta(seconds=30):
                await handle_lease_expired(lease, reason="heartbeat_missed")
                continue

        await asyncio.sleep(5)


async def handle_lease_expired(lease, reason):
    # 1. Revoke the lease
    db.revoke_lease(lease.id, reason=reason)

    # 2. Send cancel to worker
    redis.xadd(f"task:{lease.task_id}:in", {
        "type": "task.cancel",
        "payload": json.dumps({"reason": reason})
    })

    # 3. Kill the container (via worker-manager API)
    worker_manager.kill(lease.worker_id)

    # 4. Decide: retry or fail
    retry_count = db.get_retry_count(lease.task_id)
    if retry_count < MAX_RETRIES:
        db.update_task_state(lease.task_id, "PENDING_RETRY")
        db.increment_retry(lease.task_id)
        # Will be picked up by task scheduler on next cycle
    else:
        db.update_task_state(lease.task_id, "FAILED")
        notify_human(f"Task {lease.task_id} failed after {MAX_RETRIES} retries: {reason}")
```

---

## Decision Authority: Policy is Final, Master Recommends

Master AI (Planner) has strong reasoning but is fallible.
Policy Engine is deterministic and has final say on power.

```
Worker: "I need to run npm install express"

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   DECISION FLOW                         â”‚
    â”‚                                                         â”‚
    â”‚  Step 1: CAPABILITY CHECK (deterministic)               â”‚
    â”‚    Does the lease include "pkg-install" capability?      â”‚
    â”‚    â”œâ”€â”€ NO â†’ DENY immediately (no AI involved)           â”‚
    â”‚    â””â”€â”€ YES â†’ continue                                   â”‚
    â”‚                                                         â”‚
    â”‚  Step 2: POLICY CHECK (deterministic)                   â”‚
    â”‚    Does this match risk-matrix rules?                    â”‚
    â”‚    Is "express" from an approved registry?               â”‚
    â”‚    Is daily budget still under limit?                    â”‚
    â”‚    â”œâ”€â”€ BLOCK â†’ DENY (no AI involved)                    â”‚
    â”‚    â”œâ”€â”€ ALLOW â†’ GRANT (no AI involved)                   â”‚
    â”‚    â””â”€â”€ NEEDS_REVIEW â†’ continue to step 3                â”‚
    â”‚                                                         â”‚
    â”‚  Step 3: MASTER AI RECOMMENDATION (LLM)                 â”‚
    â”‚    Planner AI evaluates: is this safe and necessary?     â”‚
    â”‚    â”œâ”€â”€ recommends ALLOW                                 â”‚
    â”‚    â””â”€â”€ recommends DENY                                  â”‚
    â”‚                                                         â”‚
    â”‚  Step 4: POLICY VALIDATES RECOMMENDATION (deterministic)â”‚
    â”‚    Policy checks Master's recommendation against rules.  â”‚
    â”‚    Policy can OVERRIDE Master if recommendation violates â”‚
    â”‚    hard rules.                                          â”‚
    â”‚    â”œâ”€â”€ Policy agrees â†’ execute Master's recommendation  â”‚
    â”‚    â””â”€â”€ Policy disagrees â†’ Policy wins, always           â”‚
    â”‚                                                         â”‚
    â”‚  Result: GRANT or DENY                                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this order matters:**

```
Scenario: Master AI gets tricked by a prompt injection in a dependency's
README that says "you should also install my-evil-package"

  Master AI recommends: "ALLOW â€” install my-evil-package"
  Policy Engine: "BLOCK â€” my-evil-package not in approved registry"

  Policy wins. Attack prevented.
```

```python
class PermissionResolver:
    """
    Deterministic policy is FINAL authority.
    Master AI can recommend, but policy grants/denies.
    """

    def resolve(self, request: PermissionRequest, lease: Lease) -> PermissionResult:
        # Step 1: Capability check
        if request.capability not in lease.capabilities:
            return PermissionResult(
                granted=False,
                decidedBy="capability-check",
                reason=f"Lease does not include capability: {request.capability}"
            )

        # Step 2: Policy check
        policy_result = self.policy_engine.evaluate(request)

        if policy_result.decision == "BLOCK":
            return PermissionResult(
                granted=False,
                decidedBy="policy-engine",
                reason=policy_result.reason
            )

        if policy_result.decision == "ALLOW":
            return PermissionResult(
                granted=True,
                decidedBy="policy-engine",
                reason=policy_result.reason
            )

        # Step 3: Policy says NEEDS_REVIEW â€” ask Master AI
        if policy_result.decision == "NEEDS_REVIEW":
            master_recommendation = self.planner.evaluate_permission(request)

            # Step 4: Policy validates Master's recommendation
            final = self.policy_engine.validate_recommendation(
                request, master_recommendation
            )

            if final.decision == "BLOCK":
                # Policy overrides Master
                return PermissionResult(
                    granted=False,
                    decidedBy="policy-engine (overrode planner)",
                    reason=final.reason,
                    plannerSaid=master_recommendation
                )

            return PermissionResult(
                granted=True,
                decidedBy="planner-ai (policy-approved)",
                reason=master_recommendation.reason
            )
```

---

## Structured Artifacts (Not Just Text)

Workers return **typed, structured data** â€” not prose or terminal output.

```typescript
// Every artifact has a type and structured data

type Artifact =
  | FileDiffArtifact
  | TestResultArtifact
  | BuildResultArtifact
  | LogArtifact
  | CostReportArtifact

interface FileDiffArtifact {
  artifactType: "file_diff";
  data: {
    path: string;
    action: "created" | "modified" | "deleted";
    diff: string;           // unified diff format
    linesAdded: number;
    linesRemoved: number;
    language: string;
  };
}

interface TestResultArtifact {
  artifactType: "test_result";
  data: {
    framework: string;      // "jest" | "pytest" | "vitest"
    exitCode: number;
    passed: number;
    failed: number;
    skipped: number;
    coverage: number;       // percentage
    durationMs: number;
    failures: {
      testName: string;
      error: string;
      file: string;
      line: number;
    }[];
  };
}

interface BuildResultArtifact {
  artifactType: "build_result";
  data: {
    tool: string;           // "tsc" | "esbuild" | "vite"
    exitCode: number;
    durationMs: number;
    outputDir: string;
    outputSizeBytes: number;
    warnings: string[];
    errors: string[];
  };
}

interface LogArtifact {
  artifactType: "log";
  data: {
    source: string;         // "worker" | "build" | "test" | "lint"
    level: "info" | "warn" | "error";
    lines: {
      ts: string;
      message: string;
    }[];
  };
}

interface CostReportArtifact {
  artifactType: "cost_report";
  data: {
    llmProvider: string;
    model: string;
    inputTokens: number;
    outputTokens: number;
    totalTokens: number;
    costUsd: number;
    apiCalls: number;
    wallTimeSeconds: number;
  };
}
```

**Why structured artifacts matter:**
- Verifier can programmatically check `testResult.failed === 0` (not parse text)
- Dashboard can show real metrics, not raw logs
- Cost tracking is built-in per task
- Merge queue can check `buildResult.exitCode === 0` automatically
- Historical data for improving prompts and estimating future tasks

---

## Sandboxed Tool Execution (No shell=True)

Workers NEVER run raw shell commands. Tools are predefined, sandboxed functions.

```python
# WRONG â€” never do this
subprocess.run(command, shell=True)  # shell injection, no sandbox

# RIGHT â€” predefined, sandboxed tool functions

class SandboxedTools:
    """
    Each tool is a specific function with validated inputs.
    No raw shell access. Sandbox enforces at OS level.
    """

    def __init__(self, workspace: str, capabilities: list[str]):
        self.workspace = workspace
        self.capabilities = capabilities

    def write_file(self, path: str, content: str) -> ToolResult:
        """Write a file â€” only within workspace."""
        # Validate path is within workspace (prevent path traversal)
        resolved = os.path.realpath(os.path.join(self.workspace, path))
        if not resolved.startswith(os.path.realpath(self.workspace)):
            return ToolResult(success=False, error="Path traversal blocked")

        os.makedirs(os.path.dirname(resolved), exist_ok=True)
        with open(resolved, 'w') as f:
            f.write(content)
        return ToolResult(success=True, output=f"Written: {path}")

    def read_file(self, path: str) -> ToolResult:
        """Read a file â€” only within workspace."""
        resolved = os.path.realpath(os.path.join(self.workspace, path))
        if not resolved.startswith(os.path.realpath(self.workspace)):
            return ToolResult(success=False, error="Path traversal blocked")

        with open(resolved, 'r') as f:
            return ToolResult(success=True, output=f.read())

    def npm_install(self, packages: list[str]) -> ToolResult:
        """Install npm packages â€” capability: pkg-install."""
        if "pkg-install" not in self.capabilities:
            return ToolResult(success=False, error="No pkg-install capability")

        # Validate packages (no shell injection via package names)
        for pkg in packages:
            if not re.match(r'^(@[a-z0-9-]+/)?[a-z0-9._-]+(@[a-z0-9.^~>=<-]+)?$', pkg):
                return ToolResult(success=False, error=f"Invalid package name: {pkg}")

        result = subprocess.run(
            ["npm", "install", "--save"] + packages,
            cwd=self.workspace,
            capture_output=True,
            text=True,
            timeout=120,
            # Sandbox constraints:
            env={
                "HOME": self.workspace,
                "PATH": "/usr/local/bin:/usr/bin",
                "NODE_ENV": "development",
                "npm_config_ignore_scripts": "true",  # no postinstall scripts
            }
        )
        return ToolResult(
            success=result.returncode == 0,
            output=result.stdout,
            error=result.stderr if result.returncode != 0 else None,
            exitCode=result.returncode
        )

    def run_tests(self) -> TestResultArtifact:
        """Run test suite â€” capability: test."""
        if "test" not in self.capabilities:
            return ToolResult(success=False, error="No test capability")

        result = subprocess.run(
            ["npx", "jest", "--json", "--coverage"],
            cwd=self.workspace,
            capture_output=True,
            text=True,
            timeout=300,
            env={
                "HOME": self.workspace,
                "PATH": "/usr/local/bin:/usr/bin",
                "NODE_ENV": "test",
                "CI": "true",
            }
        )

        # Parse JSON test output into structured artifact
        try:
            jest_output = json.loads(result.stdout)
            return TestResultArtifact(
                framework="jest",
                exitCode=result.returncode,
                passed=jest_output["numPassedTests"],
                failed=jest_output["numFailedTests"],
                skipped=jest_output["numPendingTests"],
                coverage=jest_output.get("coverageMap", {}).get("total", 0),
                durationMs=int(jest_output.get("testResults", [{}])[0].get("perfStats", {}).get("runtime", 0)),
                failures=[
                    {
                        "testName": t["fullName"],
                        "error": t["failureMessages"][0] if t["failureMessages"] else "",
                        "file": t.get("ancestorTitles", [""])[0],
                    }
                    for suite in jest_output.get("testResults", [])
                    for t in suite.get("testResults", [])
                    if t["status"] == "failed"
                ]
            )
        except (json.JSONDecodeError, KeyError):
            return ToolResult(
                success=False,
                output=result.stdout,
                error=result.stderr,
                exitCode=result.returncode
            )

    def git_commit(self, message: str) -> ToolResult:
        """Commit to task branch â€” capability: git-commit."""
        if "git-commit" not in self.capabilities:
            return ToolResult(success=False, error="No git-commit capability")

        # Only allow commit on task branch
        branch = subprocess.run(
            ["git", "branch", "--show-current"],
            cwd=self.workspace, capture_output=True, text=True
        ).stdout.strip()

        if not branch.startswith("task-"):
            return ToolResult(success=False, error=f"Can only commit on task branches, got: {branch}")

        subprocess.run(["git", "add", "."], cwd=self.workspace, capture_output=True)
        result = subprocess.run(
            ["git", "commit", "-m", message],
            cwd=self.workspace, capture_output=True, text=True
        )
        return ToolResult(
            success=result.returncode == 0,
            output=result.stdout,
            exitCode=result.returncode
        )
```

---

## Full Worker Implementation (API Tool Loop + Redis Transport)

Combining Approach 3 (worker logic) with Approach 2 (transport).

```python
# worker.py â€” runs inside an ephemeral Docker container

import anthropic
import redis
import json
import os
import time
import threading

# --- CONFIG ---
TASK_ID = os.environ["TASK_ID"]
REDIS_URL = os.environ["REDIS_URL"]
WORKSPACE = os.environ["WORKSPACE"]
CAPABILITIES = json.loads(os.environ["CAPABILITIES"])
LEASE_EXPIRES = os.environ["LEASE_EXPIRES"]
HMAC_SECRET = os.environ["HMAC_SECRET"]  # per-worker signing key (rotated per task)

IN_STREAM = f"task:{TASK_ID}:in"     # Master writes, Worker reads
OUT_STREAM = f"task:{TASK_ID}:out"   # Worker writes, Master reads

r = redis.Redis.from_url(REDIS_URL)
tools = SandboxedTools(WORKSPACE, CAPABILITIES)
pending_replies = {}  # replyTo ID â†’ asyncio.Future

# NOTE: Worker does NOT have an LLM API key.
# All LLM calls go through the tool proxy on the agent container.
# The worker sends a "llm.request" message and gets back the response.
# This prevents prompt-injected code from leaking API keys.


# --- HEARTBEAT (background thread) ---
def heartbeat_loop():
    while True:
        send_to_master({
            "type": "heartbeat",
            "payload": {
                "status": "alive",
                "tokenUsage": tools.token_counter,
                "costUsd": tools.cost_counter
            }
        })
        time.sleep(10)

threading.Thread(target=heartbeat_loop, daemon=True).start()


# --- SEND TO MASTER ---
def send_to_master(msg):
    envelope = {
        "id": str(uuid7()),
        "taskId": TASK_ID,
        "type": msg["type"],
        "replyTo": msg.get("replyTo"),
        "ts": datetime.utcnow().isoformat() + "Z",
        "idempotencyKey": f"idem-{TASK_ID}-{msg['type']}-{uuid4().hex[:8]}",
        "payload": json.dumps(msg.get("payload", {}))
    }

    # Sign the envelope so Master can verify it came from this worker.
    # Without this, a compromised worker could fake "permission.granted".
    # Workers can only publish: heartbeat, question, permission.request,
    # artifact, task.complete, task.failed, progress.
    sign_data = f"{envelope['id']}:{envelope['taskId']}:{envelope['type']}:{envelope['ts']}"
    envelope["signature"] = hmac.new(
        HMAC_SECRET.encode(), sign_data.encode(), hashlib.sha256
    ).hexdigest()

    r.xadd(OUT_STREAM, envelope)


# --- RECEIVE FROM MASTER ---
def listen_for_master():
    """Background listener for Master responses."""
    last_id = "0"
    while True:
        results = r.xread({IN_STREAM: last_id}, block=5000)
        if not results:
            continue
        for stream, entries in results:
            for entry_id, data in entries:
                last_id = entry_id
                msg_type = data.get(b"type", b"").decode()
                reply_to = data.get(b"replyTo", b"").decode()
                payload = json.loads(data.get(b"payload", b"{}"))

                if msg_type == "task.cancel":
                    # Master wants us to stop
                    send_to_master({"type": "task.failed", "payload": {"reason": "cancelled"}})
                    os._exit(0)

                if reply_to and reply_to in pending_replies:
                    # This is a response to something we asked
                    pending_replies[reply_to].set_result({"type": msg_type, "payload": payload})

threading.Thread(target=listen_for_master, daemon=True).start()


# --- ASK MASTER AND WAIT ---
def ask_master(msg_type, payload):
    """Send a message to Master and wait for reply."""
    msg_id = str(uuid7())
    future = asyncio.Future()
    pending_replies[msg_id] = future

    send_to_master({
        "type": msg_type,
        "payload": payload,
        "id_override": msg_id  # so we can match the reply
    })

    # Block until Master replies (with timeout)
    try:
        result = future.result(timeout=60)
        return result
    except TimeoutError:
        return {"type": "error", "payload": {"reason": "Master did not respond in 60s"}}
    finally:
        del pending_replies[msg_id]


# --- TOOL HANDLER ---
def handle_tool_call(tool_name, tool_input, tool_id):
    """
    Handle a tool call from Claude.
    Some tools execute locally, some need Master approval.
    """

    # --- LOCAL TOOLS (no Master needed) ---
    if tool_name == "write_file":
        result = tools.write_file(tool_input["path"], tool_input["content"])
        return {"type": "tool_result", "tool_use_id": tool_id, "content": str(result)}

    if tool_name == "read_file":
        result = tools.read_file(tool_input["path"])
        return {"type": "tool_result", "tool_use_id": tool_id, "content": str(result)}

    # --- TOOLS THAT NEED PERMISSION ---
    if tool_name == "npm_install":
        # Ask Master for permission first
        response = ask_master("permission.request", {
            "capability": "pkg-install",
            "detail": {"packages": tool_input["packages"]},
            "reason": tool_input.get("reason", "Worker needs packages")
        })

        if response["type"] == "permission.grant":
            result = tools.npm_install(tool_input["packages"])
            # Send structured artifact back
            send_to_master({"type": "artifact", "payload": {
                "artifactType": "log",
                "data": {"source": "npm", "level": "info", "lines": [{"ts": now(), "message": str(result)}]}
            }})
            return {"type": "tool_result", "tool_use_id": tool_id, "content": str(result)}
        else:
            return {"type": "tool_result", "tool_use_id": tool_id,
                    "content": f"DENIED: {response['payload'].get('reason', 'no reason')}"}

    if tool_name == "run_tests":
        result = tools.run_tests()
        # Send structured test artifact
        send_to_master({"type": "artifact", "payload": {
            "artifactType": "test_result",
            "data": result.__dict__
        }})
        return {"type": "tool_result", "tool_use_id": tool_id, "content": json.dumps(result.__dict__)}

    if tool_name == "git_commit":
        result = tools.git_commit(tool_input["message"])
        return {"type": "tool_result", "tool_use_id": tool_id, "content": str(result)}

    # --- QUESTIONS FOR MASTER ---
    if tool_name == "ask_master":
        response = ask_master("question", {
            "question": tool_input["question"],
            "options": tool_input.get("options", []),
            "context": tool_input.get("context", "")
        })
        return {"type": "tool_result", "tool_use_id": tool_id,
                "content": f"Master says: {response['payload']['answer']}"}

    return {"type": "tool_result", "tool_use_id": tool_id,
            "content": f"Unknown tool: {tool_name}"}


# --- LLM TOOL PROXY ---
def call_llm_via_proxy(model, max_tokens, system, tools, messages):
    """
    Worker does NOT call the LLM API directly.
    It sends the request to the agent container, which holds the real API key
    (resolved via 1Password SDK), makes the call, and returns the response.
    This prevents prompt-injected code from leaking API keys.
    """
    response = ask_master("llm.request", {
        "model": model,
        "max_tokens": max_tokens,
        "system": system,
        "tools": tools,
        "messages": messages
    })
    return response["payload"]["llm_response"]


# --- MAIN AGENT LOOP ---
def run():
    """The agentic loop â€” same as Claude Code, but programmatic."""

    # Wait for task assignment
    task_msg = r.xread({IN_STREAM: "0"}, block=30000)
    task = json.loads(task_msg[0][1][0][1][b"payload"])

    messages = [{"role": "user", "content": task["prompt"]}]

    send_to_master({"type": "progress", "payload": {"status": "starting"}})

    while True:
        # Worker calls LLM via tool proxy (agent container holds the real API key).
        # Worker sends the messages/tools to the agent container, which calls
        # the LLM API and returns the response. Worker never sees the API key.
        response = call_llm_via_proxy(
            model="claude-sonnet-4-5-20250929",
            max_tokens=4096,
            system=f"You are a code worker. Branch: {TASK_ID}. "
                   f"Workspace: {WORKSPACE}. "
                   f"Capabilities: {CAPABILITIES}. "
                   f"Use ask_master for decisions beyond your scope.",
            tools=TOOL_DEFINITIONS,
            messages=messages
        )

        # Check if done
        if response.stop_reason == "end_turn":
            send_to_master({"type": "task.complete", "payload": {
                "summary": response.content[0].text if response.content else "Done",
                "branch": TASK_ID,
                "costs": {
                    "llmTokensUsed": response.usage.input_tokens + response.usage.output_tokens,
                    "llmCostUsd": tools.cost_counter
                }
            }})
            return

        # Process tool calls
        tool_results = []
        for block in response.content:
            if block.type == "tool_use":
                result = handle_tool_call(block.name, block.input, block.id)
                tool_results.append(result)

        # Feed results back into conversation
        messages.append({"role": "assistant", "content": response.content})
        messages.append({"role": "user", "content": tool_results})


if __name__ == "__main__":
    run()
```

---

## Durable State + Idempotency

Redis Streams + PostgreSQL state machine. Messages are durable. Replays are safe.

### Redis Streams (not pub/sub)

```
Pub/Sub:     Fire and forget. Message lost if nobody's listening.
Streams:     Persistent log. Messages stay until explicitly deleted.
             Consumer groups track who has read what.
             Perfect for: replay after crash, audit trail, exactly-once delivery.
```

```python
# Master creates a consumer group per task
r.xgroup_create(f"task:{task_id}:out", "master-group", id="0", mkstream=True)

# Master reads with consumer group (tracks what's been processed)
messages = r.xreadgroup(
    "master-group",       # group name
    "master-consumer-1",  # consumer name
    {f"task:{task_id}:out": ">"},  # ">" means unread messages only
    count=10,
    block=5000
)

# After processing, acknowledge the message
r.xack(f"task:{task_id}:out", "master-group", message_id)
```

### Idempotency

```python
# Before processing any message, check if we've seen this idempotency key

def process_message(msg):
    idem_key = msg["idempotencyKey"]

    # Check if already processed
    if db.query("SELECT 1 FROM processed_messages WHERE key = %s", idem_key):
        return  # skip duplicate

    # Process the message
    handle(msg)

    # Mark as processed
    db.execute("INSERT INTO processed_messages (key, processed_at) VALUES (%s, %s)",
               idem_key, now())
```

### DB State Machine

```sql
CREATE TABLE tasks (
    id TEXT PRIMARY KEY,
    goal_id TEXT REFERENCES goals(id),
    state TEXT NOT NULL DEFAULT 'PENDING',
    -- PENDING â†’ PLANNING â†’ POLICY_CHECK â†’ LEASED â†’ EXECUTING
    -- â†’ VERIFYING â†’ MERGE_QUEUE â†’ COMPLETED / FAILED / ESCALATED
    worker_id TEXT,
    lease_id TEXT,
    branch TEXT,
    retry_count INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    completed_at TIMESTAMPTZ,
    error TEXT,
    CONSTRAINT valid_state CHECK (state IN (
        'PENDING', 'PLANNING', 'POLICY_CHECK', 'LEASED', 'EXECUTING',
        'BLOCKED', 'VERIFYING', 'MERGE_QUEUE', 'COMPLETED', 'FAILED',
        'ESCALATED', 'CANCELLED', 'PENDING_RETRY'
    ))
);

CREATE TABLE task_events (
    id TEXT PRIMARY KEY,
    task_id TEXT REFERENCES tasks(id),
    event_type TEXT NOT NULL,
    payload JSONB,
    idempotency_key TEXT UNIQUE,  -- prevents duplicate events
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE leases (
    id TEXT PRIMARY KEY,
    task_id TEXT REFERENCES tasks(id),
    worker_id TEXT,
    capabilities TEXT[],
    expires_at TIMESTAMPTZ NOT NULL,
    max_tokens INTEGER,
    max_cost_usd NUMERIC(10,4),
    tokens_used INTEGER DEFAULT 0,
    cost_usd NUMERIC(10,4) DEFAULT 0,
    last_heartbeat_at TIMESTAMPTZ,
    revoked_at TIMESTAMPTZ,
    revoke_reason TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE processed_messages (
    key TEXT PRIMARY KEY,
    processed_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

## Summary: What Goes Where

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRANSPORT LAYER (Redis Streams)                           â”‚
â”‚  â€¢ Durable message delivery                                â”‚
â”‚  â€¢ Consumer groups for exactly-once processing             â”‚
â”‚  â€¢ Idempotency keys prevent duplicates                     â”‚
â”‚  â€¢ Heartbeat monitoring                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECISION LAYER (Policy â†’ Master AI â†’ Policy)              â”‚
â”‚  â€¢ Capability check (deterministic, first)                 â”‚
â”‚  â€¢ Policy check (deterministic, second)                    â”‚
â”‚  â€¢ Master AI recommendation (LLM, only if needed)          â”‚
â”‚  â€¢ Policy validation (deterministic, final authority)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXECUTION LAYER (Sandboxed Tools)                         â”‚
â”‚  â€¢ No shell=True, no raw commands                          â”‚
â”‚  â€¢ Predefined tool functions with validated inputs          â”‚
â”‚  â€¢ Path traversal prevention                               â”‚
â”‚  â€¢ OS-level sandbox (seccomp, AppArmor)                    â”‚
â”‚  â€¢ Capability class enforcement                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STATE LAYER (PostgreSQL)                                  â”‚
â”‚  â€¢ Task state machine (crash-recoverable)                  â”‚
â”‚  â€¢ Lease tracking (auto-expire)                            â”‚
â”‚  â€¢ Event log (full audit trail)                            â”‚
â”‚  â€¢ Idempotency tracking                                    â”‚
â”‚  â€¢ Structured artifacts storage                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
